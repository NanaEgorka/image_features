{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70af95f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "493c7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "set_random_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "668e5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CroppedDataset(Dataset): # assume that all shapes are the same\n",
    "    def __init__(self, folder_path, patch_size=(32, 32)):\n",
    "        super().__init__()\n",
    "        self.folder_path = folder_path\n",
    "        self.image_pathes = os.listdir(folder_path)\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        img = plt.imread(os.path.join(self.folder_path, self.image_pathes[0]))\n",
    "        img = img[:-38] ## cut the bottom line\n",
    "        self.patches_i = int(img.shape[0] / self.patch_size[0])\n",
    "        self.patches_j = int(img.shape[1] / self.patch_size[1])\n",
    "        self.n_patches = self.patches_i * self.patches_j\n",
    "        \n",
    "        self.data = []\n",
    "        for path in self.image_pathes:\n",
    "            img = plt.imread(os.path.join(self.folder_path, path))\n",
    "            self.data.append(img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pathes) * self.n_patches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_index = index // self.n_patches\n",
    "        #img = plt.imread(os.path.join(self.folder_path, self.image_pathes[img_index]))\n",
    "        img = self.data[img_index]\n",
    "        if len(img.shape) > 2:\n",
    "            img = img[:, :, 0]\n",
    "        img = img[:-38] ## cut the bottom line\n",
    "        img = img / 255\n",
    "        \n",
    "        patch_index = index % self.n_patches ## select patch\n",
    "        patch_i = patch_index // self.patches_j\n",
    "        patch_j = patch_index % self.patches_j\n",
    "        img = img[patch_i * self.patch_size[0]:(patch_i + 1) * self.patch_size[0],\n",
    "                 patch_j * self.patch_size[1]:(patch_j + 1) * self.patch_size[1]]\n",
    "        img = img.round() ## binarization\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a91b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data2/micrographs/'\n",
    "\n",
    "dataset = CroppedDataset(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96a6cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size],\n",
    "                                                           generator=torch.Generator().manual_seed(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "756bb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 512, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = 512, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcedbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(3 * 3 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.latent_dim)\n",
    "        )\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):      \n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        #return torch.sigmoid(x).round()\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d5d5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import Output\n",
    "from tqdm.auto import trange\n",
    "from typing import Type, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module, \n",
    "    train_dataloader: torch.utils.data.DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    verbose_num_iters: int = 32,\n",
    "    device: torch.device = \"cuda\", \n",
    "    conditional: bool = False\n",
    "):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    epoch_loss_trace = []\n",
    "    \n",
    "    display()\n",
    "    out = Output()\n",
    "    display(out)\n",
    "    \n",
    "    for i, x in tqdm(enumerate(train_dataloader), leave=False, total=len(train_dataloader)):\n",
    "        x = x.unsqueeze(1).float()\n",
    "        x = x.to(device)\n",
    "        reconstructed_x = model(x)\n",
    "        loss = criterion(x, reconstructed_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss_trace.append(loss.item())\n",
    "\n",
    "        if (i + 1) % verbose_num_iters == 0:\n",
    "            with out:\n",
    "                clear_output(wait=True)\n",
    "\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.title(\"Current epoch loss\", fontsize=22)\n",
    "                plt.xlabel(\"Iteration\", fontsize=16)\n",
    "                plt.ylabel(\"Reconstruction loss\", fontsize=16)\n",
    "                plt.grid()\n",
    "                plt.plot(epoch_loss_trace)\n",
    "\n",
    "                for j in range(3):\n",
    "                    plt.subplot(2, 6, 4 + j)\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.imshow(x[j].permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n",
    "\n",
    "                    plt.subplot(2, 6, 10 + j)\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.imshow(reconstructed_x[j].permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n",
    "\n",
    "                plt.show()\n",
    "    \n",
    "    out.clear_output()\n",
    "    \n",
    "    return epoch_loss_trace\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module, \n",
    "    train_dataloader: torch.utils.data.DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    num_epochs: int = 5, \n",
    "    verbose_num_iters: int = 32,\n",
    "    device: torch.device = \"cuda\",\n",
    "    conditional: bool = False\n",
    "):\n",
    "    loss_trace = []\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epoch: \", leave=True):        \n",
    "        epoch_loss_trace = train_epoch(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            verbose_num_iters=verbose_num_iters,\n",
    "            device=device,\n",
    "            conditional=conditional\n",
    "        )\n",
    "        \n",
    "        loss_trace += epoch_loss_trace\n",
    "        \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Total training loss\", fontsize=22)\n",
    "    plt.xlabel(\"Iteration\", fontsize=16)\n",
    "    plt.ylabel(\"Reconstruction loss\", fontsize=16)\n",
    "    plt.grid()\n",
    "    plt.plot(loss_trace)\n",
    "    plt.show()\n",
    "    \n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c17479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd40f910666473781fbc3fdd68721c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c30f33977b4f4e9f71c3fb3e3f243a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e185280cd9964dfb8ba7e8b730499226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34407f9b76e4a59a32a3f3e55d83376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95478126de0f49c991b04f375353e359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(latent_dim=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "criterion = nn.MSELoss()\n",
    "train_model(model, train_dataloader, optimizer, device=device, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dc2c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    original = []\n",
    "    reconstructed = []\n",
    "    for i, x in tqdm(enumerate(loader), leave=False, total=len(loader)):\n",
    "        x = x.unsqueeze(1).float()\n",
    "        x = x.to(device)\n",
    "        reconstructed_x = model(x)\n",
    "        original.append(x)\n",
    "        reconstructed.append(reconstructed_x)\n",
    "    return original, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8f36f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original, reconstructed = predict(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d0878ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.791919580433793\n",
      "MSE: 0.1318224193607184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "print('Accuracy:', accuracy_score(torch.cat(original).cpu().detach().numpy().astype(int).flatten(),\n",
    "               torch.cat(reconstructed).round().cpu().detach().numpy().astype(int).flatten()))\n",
    "\n",
    "print('MSE:', mean_squared_error(torch.cat(original).cpu().detach().numpy().astype(int).flatten(),\n",
    "               torch.cat(reconstructed).cpu().detach().numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315e21e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
